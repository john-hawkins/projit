%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[ICCDA '23]{}{July, 2023}{GuiYang, CN}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/23/07}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Projit: An Open Source tool for Decoupled Data Science}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{John Hawkins}
\email{john.hawkins@Getting-Data-Science-Done.com}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Getting-Data-Science-Done.com}
  \city{Sydney}
  \state{NSW}
  \country{Australia}
  \postcode{2000}
}

\renewcommand{\shortauthors}{Hawkins}

\begin{abstract} 
Scientific practice has expanded to become increasingly dependent of digitial technologies,
large scale data processing and advanced analytical methods. These shifts have demanded new
methods of implementing and recording the details of scientific projects. Monolothic applications
have the advantage of a single and consistent design, however they impede the ability of users to
innovate and incrementally improve processes. We discuss the qualities of an ideal eScience framework
for building multi-stage collaboration scinetific workflows and present an open source implementation
for managing scientific processes in a decoupled fashion that permits both flexible implementation of
any stage of processing, and greater ease of meta-data analysis.
\end{abstract}


\keywords{Data Science, Experiment Tracking, Reproducible Science, Metadata Tracking}

\maketitle

\section{Introduction}

Progress in scientific research depends heavily on accurate record keeping of previous
experimental approaches and results. Failure to maintain these records impedes progress
by making it difficult to reproduce work, or imposing the costs of repeatedly testing 
failed lines of experimentation. This cost becomes particularly high in the age of the
reproducibility crisis, as a many teams running the same experiments in parallel, without
knowledge of each others work, has produced a factory line of un-reproducible results.
 
Many Machine Learning experimentation frameworks focus on the task of making experiments
easier to execute and deploy into production systems\cite{Alberti:2018,MolnerDomenech:2020}. 
To do so they are often constructed for specific combinations of technology. This is a necessary limitation 
to enable efficiency, but has the effect of limiting general applicability.
Many other eScience frameworks focus on the lineage and management of data, referred to as 
the so-called provenance problem \cite{Sahoo:2008,Conquest:2021}
The goal of the provenance frameworks is sufficient auditibiliy of data that will 
render eScience transparent and repeatable. This can be auditing of data from multiple source systems,
or it can be auditing of logs generated during data processing\cite{Ferdous2020}. Regardless of the
specific data to be audited, these frameworks focus on developing unified systems and processes so
that auditing can be easily performed over many projects.

In addition to systems for storage of data, eScience application may include facilities for orchestration
of data processes and services\cite{Subramanian2013}, analysis of results, generation of insights
and documentation. 

Frameworks for eScience will typically need to take a position on the extent to which they are domain
specific, versus general purpose. A domain specific approach that integrates multiple data sources in
a domain aware fashion can faciliate automated or assisted scientific discover\cite{Howe2008}. On the
other hand a general purpose framework facilitates multi-disciplinary collaboration and permits meta-analysis
that transcends the boundaries of disciplines.

Other frameworks and approaches in eScience focus on understanding how to do large scale collaborative science, or
facilitate meta-level learning of various kinds\cite{Hunter:2005,Liu:2023}. The better we track the
process of science as a whole, the better we can understand both how to improve scientific processes
as well as data mine the history of science for phenomena that were difficult to detect.

\section{Methodology}

We begin by discussing all desirable elements required of an open science framework. These are drawn
from observations of both how collaborative science works and the successful components of distributed
scientific endeavours. These requirements are drawn from both sciences that are typically dependent
on computational frameworks (computer science, bioinformatics, physics) and those that generally are not
(social science, psychology).

The key elements are as follows:

\begin{itemize}
 \item Sources: Provenance of Data Sources
 \item Processing: Record of Data Processing
 \item Reuse: Facilitating Reuse of Datasets
 \item Tracking: Tracking of Experiments and outputs
 \item Results: Comparison of Methods and Results
 \item Documentation: Generation of Documentation
 \item Reproducibility: Facilitate reproduction of results
 \item Meta-Analysis: Facilitation of Meta-Analysis
\end{itemize}

The elements in this list are organised in an approximately sequential manner. However, as we discuss
them below it should be apparent that there are many ways in which these elements support each other.

First and foremost, data driven projects require a access to the required \textbf{source} data 
and need to maintain records of this data provenance for \textbf{reproducibility}. 
There will typically be \textbf{processing} applied to these datasets to 
render them applicable to experimentation and analysis. An ideal tool will track the sequential 
nature of this \textbf{processing} as well as store information about the location of each resulting dataset.
The data processed in this way is then available for \textbf{reuse} across experiments and analysis,
making \textbf{results} comparable.

The centralised storage of data in a unified format allows for scripted generation of \textbf{documentation},
and facilitates easy \textbf{meta-analysis}. If the product metadata is stored in a public or open source
repository then it is possible to build tools that extract and process the data from multiple projects. It
will permit the emgergence of an ecosystem of tools that mine the history of experiments conducted on the
same or similar source data, evaluate experimental protocols or algorithms across projects and potentially
automate some forms of \textbf{meta-analysis}.
 
To achieve these advantages we require a uniform system for storing all necessary data that are inputs and outputs 
for each stage of a data science experiment. The central store permits decoupling of processes by allowing each
element of the process to be implemented and executed independently of the others.

\subsection{Projit Process}

The central design principle of \textit{projit} is that the decoupling of data science 
should be achieved through utilisation of a simple metadata store. Each aspect of 
data science work can be allowed to proceed without awareness of the structure of 
any other element as long as it can access the information it requires through this
metadata store.

\begin{figure*}
\includegraphics[scale=0.6]{./Projit_decoupled_process.drawio.png}
\caption{Projit Process for Decoupled Data Science}
\label{fig:projit}
\end{figure*}

In Figure \ref{fig:projit} we see that the core steps of data preparation, experimentation
and analysis of results all happen independently. Each of them accesses the projit store for
the information they need, storing information 

\subsection{Implementation}

Projit has been implemented as python package that functions as both a command line application
and library that can be included inside other scripts and applications. The command line application
can be used to query the project metadata in much the same way that the git application can be used.
A user can add, modify and list the collection of data assets in the project: datasets, experiments 
and results are all accessible from the command line application.

The python package can be included in a script so that the script can access the project metadata store.
This allows the script to find the location of common datasets, register themselves as an experiment
and store results once the script is complete. Programmatic interaction with the project data through
the projit API is what permits the scripts of a project to be decoupled and contribute to the project
without being aware of how any other element is structured or implemented.

\section{Case Study}

We have utilised the projit application across multiple data science projects to store reusable datasets
and the results of all experiments. Additionally, the metadata store contains infomation about the number
of times each experiment has been executed, and the execution time utilised on each run. This allows us
to generate an ad hoc script that can compare projects in terms of the data used, the number of experiments
conducted and the total execution time. This script is constructed for illustrative purposes to show that
the projit tool can permit arbitrary meta-analysis of projects through the standardised metadata stored across
git repositories.


\section{Conclusion}


\section{Acknowledgments}

\bibliographystyle{ACM-Reference-Format}
\bibliography{refs}

\end{document}
\endinput
